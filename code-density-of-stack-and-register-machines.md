# ะก Code density of stack and register machines

This appendix extends the general consideration of stack manipulation primitives provided in [$$\mathbf{2.2}$$](stacks.md#stack-manipulation-primitives) , explaining the choice of such primitives for TVM, with a comparison of stack machines and register machines in terms of the quantity of primitives used and the code density. We do this by comparing the machine code that might be generated by an optimizing compiler for the same source files, for different (abstract) stack and register machines.

It turns out that the stack machines (at least those equipped with the basic stack manipulation primitives described in [$$\mathbf{2.2.1}$$](stacks.md#2.2.1.-basic-stack-manipulation-primitives.) have far superior code density. Furthermore, the stack machines have excellent extendability with respect to additional arithmetic and arbitrary data processing operations, especially if one considers machine code automatically generated by optimizing compilers.

## C.1 Sample leaf function

We start with a comparison of machine code generated by an (imaginary) optimizing compiler for several abstract register and stack machines, corresponding to the same high-level language source code that contains the definition of a leaf function (i.e., a function that does not call any other functions). For both the register machines and stack machines, we observe the notation and conventions introduced in [$$\mathbf{2.1}$$](stacks.md#2.1-stack-calling-conventions).

### C.1.1. Sample source file for a leaf function.

The source file we consider contains one function $$f$$ that takes six (integer) arguments, $$a, b, c, d, e, f$$, and returns two (integer) values, $$x$$ and $$y$$, which are the solutions of the system of two linear equations

$$
\left\{\begin{array}{l} a x+b y=e \\ c x+d y=f \end{array}\right.
$$

The source code of the function, in a programming language similar to $$\mathrm{C}$$, might look as follows:

```
(int, int) f(int a, int b, int c, int d, int e, int f) {
int D = a*d - b*c;
int Dx = e*d - Dbx*f;
int Dy = axf - exc;
return (Dx / D, Dy / D);
}
```

We assume (cf. [$$\mathbf{2.1}$$](stacks.md#2.1-stack-calling-conventions)) that the register machines we consider accept the six parameters $$a \ldots f$$ in registers r0..r5, and return the two values $$x$$ and $$y$$ in r0 and $$r 1$$. We also assume that the register machines have 16 registers, and that the stack machine can directly access s0 to s15 by its stack manipulation primitives; the stack machine will accept the parameters in s5 to s0, and return the two values in $$\mathrm{s} 0$$ and $$\mathrm{s} 1$$, somewhat similarly to the register machine. Finally, we assume at first that the register machine is allowed to destroy values in all registers (which is slightly unfair towards the stack machine); this assumption will be revisited later.

### C.1.2. Three-address register machine.

The machine code (or rather the corresponding assembly code) for a three-address register machine (cf. [$$\mathbf{2.1.7}$$](stacks.md#2.1.7.-arguments-to-arithmetic-primitives-on-register-machines.)) might look as follows:

```
IMUL r6,r0,r3   // r6 := r0 * r3 = ad
IMUL r7,rl1,r2   // r7 := bc
SUB r6,r6,r7   // r6 := ad-bc = D 
IMUL r3,r4,r3   // r3 := ed
IMUL r1,rl1,rb5   // r1 := bf
SUB r3,r3,rl   // r3 := ed-bf = Dx
IMUL r1,r0,rb5   // r1 := af
IMUL r7,r4,r2   // r7 := ec
SUB rl,rl,r7   // r1 := af-ec = Dy
IDIV r0,r3,r6   // x := Dx/D
IDIV r1,r1,r6   // y := Dy/D
RET
```

We have used 12 operations and at least 23 bytes (each operation uses $$3 \times 4=$$ 12 bits to indicate the three registers involved, and at least 4 bits to indicate the operation performed; thus we need two or three bytes to encode each operation). A more realistic estimate would be 34 (three bytes for each arithmetic operation) or 31 bytes (two bytes for addition and subtraction, three bytes for multiplication and division).

### C.1.3. Two-address register machine.

The machine code for a twoaddress register machine might look as follows:

```
MOV r6,r0   // r6 := ro = a
MOV r7,r1   // r7 := b
IMUL r6,r3   // r6 := r6*r3
IMUL r7,r2   // r7 := bc
IMUL r3,rd   // r3 := de 
IMUL r1,r5   // ri := bf
SUB r6,r7   // r6 = ad-bc D
IMUL r5,r0   // r5 := af
SUB r3,r1   // r3 := de-bf = Dx
IMUL r2,rd   // r2 := ce
MOV r0,r3   // ro := Dx
SUB r5,r2   // r5 = af-ce = Dy
IDIV r0,r6   // ro := X = Dx/D 
MOV r1,r5   // r1:= Dy
IDIV r1,r6   // r1:= Dy/D
RET
```

We have used 16 operations; optimistically assuming each of them (with the exception of RET) can be encoded by two bytes, this code would require 31 bytes.$$^31$$

{% hint style="info" %}
$${ }^{31}$$It is interesting to compare this code with that generated by optimizing $$\mathrm{C}$$ compilers for the $$\mathrm{x} 86-64$$ architecture.

First of all, the integer division operation for x86-64 uses the one-address form, with the (double-length) dividend to be supplied in accumulator pair $$r 2: r 0$$. The quotient is also returned in ro. As a consequence, two single-to-double extension operations (CDQ or CQO) and at least one move operation need to be added.

Secondly, the encoding used for arithmetic and move operations is less optimistic than in our example above, requiring about three bytes per operation on average. As a result, we obtain a total of 43 bytes for 32-bit integers, and 68 bytes for 64-bit integers.
{% endhint %}

### C.1.4. One-address register machine.

The machine code for a oneaddress register machine might look as follows:

```
MOV r8,r0 // r8 := r0 = a
XCHG r1 // r0 <-> r1; r0 := b, r1 := a
MOV r6,r0 // r6 := b
IMUL r2 // r0 := r0*r2; r0 := bc
MOV r7,r0 // r7 := bc
MOV r0,r8 // r0 := a
IMUL r3 // r0 := ad
SUB r7 // r0 := ad-bc = D
XCHG r1 // r1 := D, r0 := b
IMUL r5 // r0 := bf
XCHG r3 // r0 := d, r3 := bf
IMUL r4 // r0 := de
SUB r3 // r0 := de-bf = Dx
IDIV r1 // r0 := Dx/D = x
XCHG r2 // r0 := c, r2 := x
IMUL r4 // r0 := ce
XCHG r5 // r0 := f, r5 := ce
IMUL r8 // r0 := af
SUB r5 // r0 := af-ce = Dy
IDIV r1 // r0 := Dy/D = y
MOV r1,r0 // r1 := y
MOV r0,r2 // r0 := x
RET
```

We have used 23 operations; if we assume one-byte encoding for all arithmetic operations and XCHG, and two-byte encodings for MOV, the total size of the code will be 29 bytes. Notice, however, that to obtain the compact code shown above we had to choose a specific order of computation, and made heavy use of the commutativity of multiplication. (For example, we compute $$b c$$ before $$a f$$, and $$a f-b c$$ immediately after $$a f$$.) It is not clear whether a compiler would be able to make all such optimizations by itself.

### C.1.5. Stack machine with basic stack primitives.

The machine code for a stack machine equipped with basic stack manipulation primitives described in [$$\mathbf{2.2.1}$$](stacks.md#2.2.1.-basic-stack-manipulation-primitives.) might look as follows:

```
PUSH s5 // a b c d e f a
PUSH s3 // a b c d e f a d
IMUL // a b c d e f ad
PUSH s5 // a b c d e f ad b
PUSH s5 // a b c d e f ad b c
IMUL // a b c d e f ad bc
SUB // a b c d e f ad-bc
XCHG s3 // a b c ad-bc e f d
PUSH s2 // a b c ad-bc e f d e
IMUL // a b c ad-bc e f de
XCHG s5 // a de c ad-bc e f b
PUSH s1 // a de c ad-bc e f b f
IMUL // a de c ad-bc e f bf
XCHG s1,s5 // a f c ad-bc e de bf
SUB // a f c ad-bc e de-bf
XCHG s3 // a f de-bf ad-bc e c
IMUL // a f de-bf ad-bc ec
XCHG s3 // a ec de-bf ad-bc f
XCHG s1,s4 // ad-bc ec de-bf a f
IMUL // D ec Dx af
XCHG s1 // D ec af Dx
XCHG s2 // D Dx af ec
SUB // D Dx Dy
XCHG s1 // D Dy Dx
PUSH s2 // D Dy Dx D
IDIV // D Dy x
XCHG s2 // x Dy D
IDIV // x y
RET
```

We have used 29 operations; assuming one-byte encodings for all stack operations involved (including XCHG s1, s(i)), we have used 29 code bytes as well. Notice that with one-byte encoding, the "unsystematic" operation ROT (equivalent to XCHG s1; XCHG s2) would reduce the operation and byte count to 28. This shows that such "unsystematic" operations, borrowed from Forth, may indeed reduce the code size on some occasions.

Notice as well that we have implicitly used the commutativity of multiplication in this code, computing $$d e-b f$$ instead of $$e d-b f$$ as specified in high-level language source code. If we were not allowed to do so, an extra XCHG s 1 would need to be inserted before the third IMUL, increasing the total size of the code by one operation and one byte.

The code presented above might have been produced by a rather unsophisticated compiler that simply computed all expressions and subexpressions in the order they appear, then rearranged the arguments near the top of the stack before each operation as outlined in [$$\mathbf{2.2.2}$$](stacks.md#2.2.2.-basic-stack-manipulation-primitives-suffice.) The only "manual" optimization done here involves computing ec before af; one can check that the other order would lead to slightly shorter code of 28 operations and bytes (or 29, if we are not allowed to use the commutativity of multiplication), but the ROT optimization would not be applicable.

### C.1.6. Stack machine with compound stack primitives.

A stack machine with compound stack primitives (cf. [$$\mathbf{2.2.3}$$](stacks.md#2.2.3.-compound-stack-manipulation-primitives.)) would not significantly improve code density of the code presented above, at least in terms of bytes used. The only difference is that, if we were not allowed to use commutativity of multiplication, the extra XCHG s1 inserted before the third IMUL might be combined with two previous operations XCHG s3, PUSH s2 into one compound operation PUXC s2, s3; we provide the resulting code below. To make this less redundant, we show a version of the code that computes subexpression af before $$e c$$ as specified in the original source file. We see that this replaces six operations (starting from line 15) with five other operations, and disables the ROT optimization:

```
PUSH s5 // a b c d e f a
PUSH s3 // a b c d e f a d
IMUL // a b c d e f ad
PUSH s5 // a b c d e f ad b
PUSH s5 // a b c d e f ad b c
IMUL // a b c d e f ad bc
SUB // a b c d e f ad-bc
PUXC s2,s3 // a b c ad-bc e f e d
IMUL // a b c ad-bc e f ed
XCHG s5 // a ed c ad-bc e f b
PUSH s1 // a ed c ad-bc e f b f
IMUL // a ed c ad-bc e f bf
XCHG s1,s5 // a f c ad-bc e ed bf
SUB // a f c ad-bc e ed-bf
XCHG s4 // a ed-bf c ad-bc e f
XCHG s1,s5 // e Dx c D a f
IMUL // e Dx c D af
XCHG s2 // e Dx af D c
XCHG s1,s4 // D Dx af e c
IMUL // D Dx af ec
SUB // D Dx Dy
XCHG s1 // D Dy Dx
PUSH s2 // D Dy Dx D
IDIV // D Dy x
XCHG s2 // x Dy D
IDIV // x y
RET
```

We have used a total of 27 operations and 28 bytes, the same as the previous version (with the ROT optimization). However, we did not use the commutativity of multiplication here, so we can say that compound stack manipulation primitives enable us to reduce the code size from 29 to 28 bytes.

Yet again, notice that the above code might have been generated by an unsophisticated compiler. Manual optimizations might lead to more compact code; for instance, we could use compound operations such as XCHG3 to prepare in advance not only the correct values of s0 and s1 for the next arithmetic operation, but also the value of s2 for the arithmetic operation after that. The next section provides an example of such an optimization.

### C.1.7. Stack machine with compound stack primitives and manually optimized code.

The previous version of code for a stack machine with compound stack primitives can be manually optimized as follows.

By interchanging XCHG operations with preceding XCHG, PUSH, and arithmetic operations whenever possible, we obtain code fragment XCHG s2, s6; XCHG s1,s0; XCHG 0 , s5, which can then be replaced by compound operation XCHG3 s6,s0,s5. This compound operation would admit a two-byte encoding, thus leading to 27-byte code using only 21 operations:

```
PUSH2 s5,s2 // a b c d e f a d
IMUL // a b c d e f ad
PUSH2 s5,s4 // a b c d e f ad b c
IMUL // a b c d e f ad bc
SUB // a b c d e f ad-bc
PUXC s2,s3 // a b c ad-bc e f e d
IMUL // a b c D e f ed
XCHG3 s6,s0,s5 // (same as XCHG s2,s6; XCHG s1,s0; XCHG s0,s5)
// e f c D a ed b
PUSH s5 // e f c D a ed b f
IMUL // e f c D a ed bf
SUB // e f c D a ed-bf
XCHG s4 // e Dx c D a f
IMUL // e Dx c D af
XCHG2 s4,s2 // D Dx af e c
IMUL // D Dx af ec
SUB // D Dx Dy
XCPU s1,s2 // D Dy Dx D
IDIV // D Dy x
XCHG s2 // x Dy D
IDIV // x y
RET

```

It is interesting to note that this version of stack machine code contains only 9 stack manipulation primitives for 11 arithmetic operations. It is not clear, however, whether an optimizing compiler would be able to reorganize the code in such a manner by itself.

## C.2 Comparison of machine code for sample leaf function

[Table 1](code-density-of-stack-and-register-machines.md#table-1) summarizes the properties of machine code corresponding to the same source file described in [$$\mathbf{C.1.1}$$](code-density-of-stack-and-register-machines.md#c11-sample-source-file-for-a-leaf-function), generated for a hypothetical three-address register machine (cf. [$$\mathbf{C.1.2}$$](code-density-of-stack-and-register-machines.md#c12-three-address-register-machine)), with both "optimistic" and "realistic" instruction encodings; a two-address machine (cf. [$$\mathbf{C.1.3}$$](code-density-of-stack-and-register-machines.md#c13-two-address-register-machine)); a one-address machine (cf. C.1.4); and a stack machine, similar to TVM, using either only the basic stack manipulation primitives (cf. [$$\mathbf{C.1.5}$$](code-density-of-stack-and-register-machines.md#c15-stack-machine-with-basic-stack-primitives)) or both the basic and the composite stack primitives [$$\mathbf{C.1.7}$$](code-density-of-stack-and-register-machines.md#c17-stack-machine-with-compound-stack-primitives-and-manually-optimized-code).

The meaning of the columns in [Table 1](code-density-of-stack-and-register-machines.md#table-1) is as follows:

* "Operations" - The quantity of instructions used, split into "data" (i.e., register move and exchange instructions for register machines, and stack manipulation instructions for stack machines) and "arithmetic" (instructions for adding, subtracting, multiplying and dividing integer numbers). The "total" is one more than the sum of these two, because there is also a one-byte RET instruction at the end of machine code.
* "Code bytes" - The total amount of code bytes used.
* "Opcode space" - The portion of "opcode space" (i.e., of possible choices for the first byte of the encoding of an instruction) used by data and arithmetic instructions in the assumed instruction encoding. For example, the "optimistic" encoding for the three-address machine assumes two-byte encodings for all arithmetic instructions op $$r(i)$$, $$r(j), r(k)$$. Each arithmetic instruction would then consume portion

| Machine         | Operations | Operations | Operations | Code bytes | Code bytes |    Code bytes    | Opcode space |            Opcode space           |              |
| --------------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------------: | :----------: | :-------------------------------: | :----------: |
| -               |    data    |    arith   |    total   |    data    |    arith   |       total      |     data     |               arith               |     total    |
| 3-addr. (opt.)  |      0     |     11     |     12     |      0     |     22     | $$\mathbf{2 3}$$ |  $$0 / 256$$ |      $$\mathbf{6 4 / 2 5 6}$$     | $$65 / 256$$ |
| 3-addr. (real.) |      0     |     11     |     12     |      0     |     30     | $$\mathbf{3 1}$$ |  $$0 / 256$$ | $$\mathbf{3 4} / \mathbf{2 5 6}$$ | $$35 / 256$$ |
| 2-addr.         |      4     |     11     |     16     |      8     |     22     | $$\mathbf{3 1}$$ |  $$1 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  |  $$6 / 256$$ |
| 1-addr.         |     11     |     11     |     23     |     17     |     11     | $$\mathbf{2 9}$$ | $$17 / 256$$ | $$\mathbf{6 4} / \mathbf{2 5 6}$$ | $$82 / 256$$ |
| stack (basic)   |     16     |     11     |     28     |     16     |     11     | $$\mathbf{2 8}$$ | $$64 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  | $$69 / 256$$ |
| stack (comp.)   |      9     |     11     |     21     |     15     |     11     | $$\mathbf{2 7}$$ | $$84 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  | $$89 / 256$$ |

#### Table 1:

A summary of machine code properties for hypothetical 3-address, 2-address, 1-address, and stack machines, generated for a sample leaf function (cf. C.1.1). The two most important columns, reflecting code density and extendability to other operations, are marked by bold font. Smaller values are better in both of these columns.

$$16 / 256=1 / 16$$ of the opcode space. Notice that for the stack machine we have assumed one-byte encodings for XCHG $$\mathrm{s}(i)$$, PUSH $$\mathrm{s}(i)$$ and POP $$\mathrm{s}(i)$$ in all cases, augmented by XCHG $$\mathrm{s} 1, \mathrm{~s}(i)$$ for the basic stack instructions case only. As for the compound stack operations, we have assumed two-byte encodings for PUSH3, XCHG3, XCHG2, XCPU, PUXC, PUSH2, but not for XCHG $$\mathrm{s} 1, \mathrm{~s}(i)$$.

The "code bytes" column reflects the density of the code for the specific sample source. However, "opcode space" is also important, because it reflects the extendability of the achieved density to other classes of operations (e.g., if one were to complement arithmetic operations with string manipulation operations and so on). Here the "arithmetic" subcolumn is more important than the "data" subcolumn, because no further data manipulation operations would be required for such extensions.

We see that the three-address register machine with the "optimistic" encoding, assuming two-byte encodings for all three-register arithmetic operations, achieves the best code density, requiring only 23 bytes. However, this comes at a price: each arithmetic operation consumes $$1 / 16$$ of the opcode space, so the four operations already use a quarter of the opcode space. At most 11 other operations, arithmetic or not, might be added to this architecture while preserving such high code density. On the other hand, when we consider the "realistic" encoding for the three-address machine, using twobyte encodings only for the most frequently used addition/subtraction operations (and longer encodings for less frequently used multiplication/division operations, reflecting the fact that the possible extension operations would likely fall in this class), then the three-address machine ceases to offer such attractive code density.

In fact, the two-address machine becomes equally attractive at this point: it is capable of achieving the same code size of 31 bytes as the three-address machine with the "realistic" encoding, using only $$6 / 256$$ of the opcode space for this! However, 31 bytes is the worst result in this table.

The one-address machine uses 29 bytes, slightly less than the two-address machine. However, it utilizes a quarter of the opcode space for its arithmetic operations, hampering its extendability. In this respect it is similar to the three-address machine with the "optimistic" encoding, but requires 29 bytes instead of 23 ! So there is no reason to use the one-address machine at all, in terms of extendability (reflected by opcode space used for arithmetic operations) compared to code density.

Finally, the stack machine wins the competition in terms of code density ( 27 or 28 bytes), losing only to the three-address machine with the "optimistic" encoding (which, however, is terrible in terms of extendability).

To summarize: the two-address machine and stack machine achieve the best extendability with respect to additional arithmetic or data processing instructions (using only $$1 / 256$$ of code space for each such instruction), while the stack machine additionally achieves the best code density by a small margin. The stack machine utilizes a significant part of its code space (more than a quarter) for data (i.e., stack) manipulation instructions; however, this does not seriously hamper extendability, because the stack manipulation instructions occupy a constant part of the opcode stace, regardless of all other instructions and extensions.

While one might still be tempted to use a two-address register machine, we will explain shortly (cf. C.3 why the two-address register machine offers worse code density and extendability in practice than it appears based on this table.

As for the choice between a stack machine with only basic stack manipulation primitives or one supporting compound stack primitives as well, the case for the more sophisticated stack machine appears to be weaker: it offers only one or two fewer bytes of code at the expense of using considerably more opcode space for stack manipulation, and the optimized code using these additional instructions is hard for programmers to write and for compilers to automatically generate.

### C.2.1. Register calling conventions: some registers must be preserved by functions.

Up to this point, we have considered the machine code of only one function, without taking into account the interplay between this function and other functions in the same program.

Usually a program consists of more than one function, and when a function is not a "simple" or "leaf" function, it must call other functions. Therefore, it becomes important whether a called function preserves all or at least some registers. If it preserves all registers except those used to return results, the caller can safely keep its local and temporary variables in certain registers; however, the callee needs to save all the registers it will use for its temporary values somewhere (usually into the stack, which also exists on register machines), and then restore the original values. On the other hand, if the called function is allowed to destroy all registers, it can be written in the manner described in [$$\mathbf{C.1.2}$$](code-density-of-stack-and-register-machines.md#c12-three-address-register-machine), [$$\mathbf{C.1.3}$$](code-density-of-stack-and-register-machines.md#c13-two-address-register-machine), and [$$\mathbf{C.1.4}$$](code-density-of-stack-and-register-machines.md#c14-one-address-register-machine), but the caller will now be responsible for saving all its temporary values into the stack before the call, and restoring these values afterwards.

In most cases, calling conventions for register machines require preservation of some but not all registers. We will assume that $$m \leq n$$ registers will be preserved by functions (unless they are used for return values), and that these registers are $$r(n-m) \ldots r(n-1)$$. Case $$m=0$$ corresponds to the case "the callee is free to destroy all registers" considered so far; it is quite painful for the caller. Case $$m=n$$ corresponds to the case "the callee must preserve all registers"; it is quite painful for the callee, as we will see in a moment. Usually a value of $$m$$ around $$n / 2$$ is used in practice.

The following sections consider cases $$m=0, m=8$$, and $$m=16$$ for our register machines with $$n=16$$ registers.

### C.2.2. Case $$m=0$$ : no registers to preserve.

This case has been considered and summarized in [$$\mathbf{C.2}$$](code-density-of-stack-and-register-machines.md#c2-comparison-of-machine-code-for-sample-leaf-func--tion) and [Table 1](code-density-of-stack-and-register-machines.md#table-1) above.

### C.2.3. Case $$m=n=16$$ : all registers must be preserved.

This case is the most painful one for the called function. It is especially difficult for leaf functions like the one we have been considering, which do not benefit at all from the fact that other functions preserve some registers when called-they do not call any functions, but instead must preserve all registers themselves.

In order to estimate the consequences of assuming $$m=n=16$$, we will assume that all our register machines are equipped with a stack, and with one-byte instructions PUSH $$r(i)$$ and POP $$r(i)$$, which push or pop a register into/from the stack. For example, the three-address machine code provided in [$$\mathbf{C.1.2}$$](code-density-of-stack-and-register-machines.md#c12-three-address-register-machine) destroys the values in registers $$r 2, r 3, r 6$$, and $$r 7$$; this means that the code of this function must be augmented by four instructions PUSH

| Machine         | Operations | Operations | Operations | Code bytes | Code bytes | Code bytes |   Opcode space   | Opcode space |           Opcode space          |  Opcode space |
| --------------- | ---------- | :--------: | :--------: | :--------: | :--------: | :--------: | :--------------: | :----------: | :-----------------------------: | :-----------: |
| -               | $$r$$      |    data    |    arith   |    total   |    data    |    arith   |       total      |     data     |              arith              |     total     |
| 3-addr. (opt.)  | 4          |      8     |     11     |     20     |      8     |     22     | $$\mathbf{3 1}$$ | $$32 / 256$$ |     $$\mathbf{6 4 / 2 5 6}$$    |  $$97 / 256$$ |
| 3-addr. (real.) | 4          |      8     |     11     |     20     |      8     |     30     | $$\mathbf{3 9}$$ | $$32 / 256$$ |     $$\mathbf{3 4 / 2 5 6}$$    |  $$67 / 256$$ |
| 2-addr.         | 5          |     14     |     11     |     26     |     18     |     22     | $$\mathbf{4 1}$$ | $$33 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$38 / 256$$ |
| 1-addr.         | 6          |     23     |     11     |     35     |     29     |     11     | $$\mathbf{4 1}$$ | $$49 / 256$$ |     $$\mathbf{6 4 / 2 5 6}$$    | $$114 / 256$$ |
| stack (basic)   | 0          |     16     |     11     |     28     |     16     |     11     | $$\mathbf{2 8}$$ | $$64 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$69 / 256$$ |
| stack (comp.)   | 0          |      9     |     11     |     21     |     15     |     11     | $$\mathbf{2 7}$$ | $$84 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$89 / 256$$ |

#### Table 2:

A summary of machine code properties for hypothetical 3-address, 2-address, 1address, and stack machines, generated for a sample leaf function (cf. [$$\mathbf{C.1.1}$$](code-density-of-stack-and-register-machines.md#c11-sample-source-file-for-a-leaf-function)), assuming all of the 16 registers must be preserved by called functions $$(m=n=16)$$. The new column labeled $$r$$ denotes the number of registers to be saved and restored, leading to $$2 r$$ more operations and code bytes compared to [Table 1](code-density-of-stack-and-register-machines.md#table-1). Newly-added PUSH and POP instructions for register machines also utilize $$32 / 256$$ of the opcode space. The two rows corresponding to stack machines remain unchanged.

r2; PUSH r3; PUSH r6; PUSH $$r 7$$ at the beginning, and by four instructions POP r7; POP r6; POP r3; POP r2 right before the RET instruction, in order to restore the original values of these registers from the stack. These four additional PUSH/POP pairs would increase the operation count and code size in bytes by $$4 \times 2=8$$. A similar analysis can be done for other register machines as well, leading to [Table 2](code-density-of-stack-and-register-machines.md#table-2).

We see that under these assumptions the stack machines are the obvious winners in terms of code density, and are in the winning group with respect to extendability.

### C.2.4. Case $$m=8, n=16$$ : registers r8...r15 must be preserved.

The analysis of this case is similar to the previous one. The results are summarized in [Table 3](code-density-of-stack-and-register-machines.md#table-3).

Notice that the resulting table is very similar to [Table 1](code-density-of-stack-and-register-machines.md#table-1), apart from the "Opcode space" columns and the row for the one-address machine. Therefore, the conclusions of [$$\mathbf{C.2}$$](code-density-of-stack-and-register-machines.md#c2-comparison-of-machine-code-for-sample-leaf-func--tion) still apply in this case, with some minor modifications. We must emphasize, however, that these conclusions are valid only for leaf functions, i.e., functions that do not call other functions. Any program aside from the very simplest will have many non-leaf functions, especially if we are minimizing resulting machine code size (which prevents inlining of functions in most cases).

### C.2.5. A fairer comparison using a binary code instead of a byte code.

The reader may have noticed that our preceding discussion of $$k$$ -

| Machine         | r | Operations | Operations | Operations | Code bytes | Code bytes |    Code bytes    | Opcode space |            Opcode space           |  Opcode space |
| --------------- | - | :--------: | :--------: | :--------: | :--------: | :--------: | :--------------: | :----------: | :-------------------------------: | :-----------: |
| -               | - |    data    |    arith   |    total   |    data    |    arith   |       total      |     data     |               arith               |     total     |
| 3-addr. (opt.)  | 0 |      0     |     11     |     12     |      0     |     22     | $$\mathbf{2 3}$$ | $$32 / 256$$ |      $$\mathbf{6 4 / 2 5 6}$$     |  $$97 / 256$$ |
| 3-addr. (real.) | 0 |      0     |     11     |     12     |      0     |     30     | $$\mathbf{3 1}$$ | $$32 / 256$$ | $$\mathbf{3 4} / \mathbf{2 5 6}$$ |  $$67 / 256$$ |
| 2-addr.         | 0 |      4     |     11     |     16     |      8     |     22     | $$\mathbf{3 1}$$ | $$33 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  |  $$38 / 256$$ |
| 1-addr.         | 1 |     13     |     11     |     25     |     19     |     11     | $$\mathbf{3 1}$$ | $$49 / 256$$ |      $$\mathbf{6 4 / 2 5 6}$$     | $$114 / 256$$ |
| stack (basic)   | 0 |     16     |     11     |     28     |     16     |     11     | $$\mathbf{2 8}$$ | $$64 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  |  $$69 / 256$$ |
| stack (comp.)   | 0 |      9     |     11     |     21     |     15     |     11     | $$\mathbf{2 7}$$ | $$84 / 256$$ |  $$\mathbf{4} / \mathbf{2 5 6}$$  |  $$89 / 256$$ |

#### Table 3:

A summary of machine code properties for hypothetical 3-address, 2-address, 1-address and stack machines, generated for a sample leaf function (cf. [$$\mathbf{C.1.1}$$](code-density-of-stack-and-register-machines.md#c11-sample-source-file-for-a-leaf-function)), assuming that only the last 8 of the 16 registers must be preserved by called functions $$(m=8$$, $$n=16$$ ). This table is similar to [Table 2](code-density-of-stack-and-register-machines.md#table-2) , but has smaller values of $$r$$.

address register machines and stack machines depended very much on our insistence that complete instructions be encoded by an integer number of bytes. If we had been allowed to use a "bit" or "binary code" instead of a byte code for encoding instructions, we could more evenly balance the opcode space used by different machines. For instance, the opcode of SUB for a threeaddress machine had to be either 4-bit (good for code density, bad for opcode space) or 12-bit (very bad for code density), because the complete instruction has to occupy a multiple of eight bits (e.g., 16 or 24 bits), and $$3 \cdot 4=12$$ of those bits have to be used for the three register names.

Therefore, let us get rid of this restriction.

Now that we can use any number of bits to encode an instruction, we can choose all opcodes of the same length for all the machines considered. For instance, all arithmetic instructions can have 8-bit opcodes, as the stack machine does, using $$1 / 256$$ of the opcode space each; then the three-address register machine will use 20 bits to encode each complete arithmetic instruction. All MOVs, XCHGs, PUSHes, and POPs on register machines can be assumed to have 4-bit opcodes, because this is what we do for the most common stack manipulation primitives on a stack machine. The results of these changes are shown in [Table 4](code-density-of-stack-and-register-machines.md#table-4).

We can see that the performance of the various machines is much more balanced, with the stack machine still the winner in terms of the code density, but with the three-address machine enjoying the second place it really merits. If we were to consider the decoding speed and the possibility of parallel execution of instructions, we would have to choose the three-address machine, because it uses only 12 instructions instead of 21.

| Machine       |  r  | Operations | Operations | Operations | Code bytes | Code bytes |      Code bytes      | Opcode space |           Opcode space          |              |
| ------------- | :-: | :--------: | :--------: | :--------: | :--------: | :--------: | :------------------: | :----------: | :-----------------------------: | :----------: |
| -             |  -  |    data    |    arith   |    total   |    data    |    arith   |         total        |     data     |              arith              |     total    |
| 3-addr.       |  0  |      0     |     11     |     12     |      0     |    27.5    | $$\mathbf{2 8 . 5}$$ | $$64 / 256$$ |      $$\mathbf{4 / 2 5 6}$$     | $$69 / 256$$ |
| 2-addr.       |  0  |      4     |     11     |     16     |      6     |     22     |   $$\mathbf{2 9}$$   | $$64 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$69 / 256$$ |
| 1-addr.       |  1  |     13     |     11     |     25     |     16     |    16.5    | $$\mathbf{3 2 . 5}$$ | $$64 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$69 / 256$$ |
| stack (basic) |  0  |     16     |     11     |     28     |     16     |     11     |   $$\mathbf{2 8}$$   | $$64 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$69 / 256$$ |
| stack (comp.) |  0  |      9     |     11     |     21     |     15     |     11     |   $$\mathbf{2 7}$$   | $$84 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$89 / 256$$ |

#### Table 4:

A summary of machine code properties for hypothetical 3-address, 2-address, 1-address and stack machines, generated for a sample leaf function (cf. [$$\mathbf{C.1.1}$$](code-density-of-stack-and-register-machines.md#c11-sample-source-file-for-a-leaf-function)), assuming that only 8 of the 16 registers must be preserved by functions $$(m=8, n=16)$$. This time we can use fractions of bytes to encode instructions, so as to match opcode space used by different machines. All arithmetic instructions have 8-bit opcodes, all data/stack manipulation instructions have 4-bit opcodes. In other respects this table is similar to [Table 3](code-density-of-stack-and-register-machines.md#table-3)

## C.3 Sample non-leaf function

This section compares the machine code for different register machines for a sample non-leaf function. Again, we assume that either $$m=0, m=8$$, or $$m=16$$ registers are preserved by called functions, with $$m=8$$ representing the compromise made by most modern compilers and operating systems.

### C.3.1. Sample source code for a non-leaf function.

A sample source file may be obtained by replacing the built-in integer type with a custom Rational type, represented by a pointer to an object in memory, in our function for solving systems of two linear equations (cf. [$$\mathbf{C.1.1}$$](code-density-of-stack-and-register-machines.md#c11-sample-source-file-for-a-leaf-function)):

```
struct Rational;
typedef struct Rational *num;
extern num r_add(num, num);
extern num r_sub(num, num);
extern num r_mul(num, num);
extern num r_div(num, num);
(num, num) r_f(num a, num b, num c, num d, num e, num f) {
num D = r_sub(r_mul(a, d), r_mul(b, c)); // a*d-b*c
num Dx = r_sub(r_mul(e, d), r_mul(b, f)); // e*d-b*f
num Dy = r_sub(r_mul(a, f), r_mul(e, c)); // a*f-e*c
return (r_div(Dx, D), r_div(Dy, D)); // Dx/D, Dy/D
}
```

We will ignore all questions related to allocating new objects of type Rational in memory (e.g., in heap), and to preventing memory leaks. We may assume that the called subroutines $$r_{-}$$sub, $$r_{-} m u l$$, and so on allocate new objects simply by advancing some pointer in a pre-allocated buffer, and that unused objects are later freed by a garbage collector, external to the code being analysed.

Rational numbers will now be represented by pointers, addresses, or references, which will fit into registers of our hypothetical register machines or into the stack of our stack machines. If we want to use TVM as an instance of these stack machines, we should use values of type Cell to represent such references to objects of type Rational in memory.

We assume that subroutines (or functions) are called by a special CALL instruction, which is encoded by three bytes, including the specification of the function to be called (e.g., the index in a "global function table").

### C.3.2. Three-address and two-address register machines, $$m=0$$ preserved registers.

Because our sample function does not use built-in arithmetic instructions at all, compilers for our hypothetical three-address and two-address register machines will produce the same machine code. Apart from the previously introduced PUSH $$r(i)$$ and POP $$r(i)$$ one-byte instructions, we assume that our two- and three-address machines support the following two-byte instructions: MOV $$r(i), \mathrm{s}(j)$$, MOV $$\mathrm{s}(j), r(i)$$, and XCHG $$r(i), \mathrm{s}(j)$$, for $$0 \leq i, j \leq 15$$. Such instructions occupy only $$3 / 256$$ of the opcode space, so their addition seems quite natural.

We first assume that $$m=0$$ (i.e., that all subroutines are free to destroy the values of all registers). In this case, our machine code for r\_f does not have to preserve any registers, but has to save all registers containing useful values into the stack before calling any subroutines. A size-optimizing compiler might produce the following code:

```
PUSH r4 // STACK: e
PUSH r1 // STACK: e b
PUSH r0 // .. e b a
PUSH r6 // .. e b a f
PUSH r2 // .. e b a f c
PUSH r3 // .. e b a f c d
MOV r0,r1 // b
MOV r1,r2 // c
CALL r_mul // bc
PUSH r0 // .. e b a f c d bc
MOV r0,s4 // a
MOV r1,s1 // d
CALL r_mul // ad
POP r1 // bc; .. e b a f c d
CALL r_sub // D:=ad-bc
XCHG r0,s4 // b ; .. e D a f c d
MOV r1,s2 // f
CALL r_mul // bf
POP r1 // d ; .. e D a f c
PUSH r0 // .. e D a f c bf
MOV r0,s5 // e
CALL r_mul // ed
POP r1 // bf; .. e D a f c
CALL r_sub // Dx:=ed-bf
XCHG r0,s4 // e ; .. Dx D a f c
POP r1 // c ; .. Dx D a f
CALL r_mul // ec
XCHG r0,s1 // a ; .. Dx D ec f
POP r1 // f ; .. Dx D ec
CALL r_mul // af
POP r1 // ec; .. Dx D
CALL r_sub // Dy:=af-ec
XCHG r0,s1 // Dx; .. Dy D
MOV r1,s0 // D
CALL r_div // x:=Dx/D
XCHG r0,s1 // Dy; .. x D
POP r1 // D ; .. x
CALL r_div // y:=Dy/D
MOV r1,r0 // y
POP r0 // x ; ..
RET

```

We have used 41 instructions: 17 one-byte (eight PUSH/POP pairs and one RET), 13 two-byte (MOV and XCHG; out of them 11 "new" ones, involving the stack), and 11 three-byte (CALL), for a total of $$17 \cdot 1+13 \cdot 2+11 \cdot 3=76$$ bytes. $${ }^{32}$$

{% hint style="info" %}
$${ }^{32}$$Code produced for this function by an optimizing compiler for $$\mathrm{x} 86-64$$ architecture with size-optimization enabled actually occupied 150 bytes, due mostly to the fact that actual instruction encodings are about twice as long as we had optimistically assumed.
{% endhint %}

### C.3.3. Three-address and two-address register machines, $$m=8$$ preserved registers.

Now we have eight registers, r8 to r15, that are preserved by subroutine calls. We might keep some intermediate values there instead of pushing them into the stack. However, the penalty for doing so consists in a PUSH/POP pair for every such register that we choose to use, because our function is also required to preserve its original value. It seems that using these registers under such a penalty does not improve the density of the code, so the optimal code for three- and two-address machines for $$m=8$$ preserved registers is the same as that provided in [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers) with a total of 42 instructions and 74 code bytes.

### C.3.4. Three-address and two-address register machines, $$m=16$$ preserved registers.

This time all registers must be preserved by the subroutines, excluding those used for returning the results. This means that our code must preserve the original values of $$r 2$$ to $$r 5$$, as well as any other registers it uses for temporary values. A straightforward way of writing the code of our subroutine would be to push registers r2 up to, say, r8 into the stack, then perform all the operations required, using $$r 6-r 8$$ for intermediate values, and finally restore registers from the stack. However, this would not optimize code size. We choose another approach:

```
PUSH r0 // STACK: a
PUSH r1 // STACK: a b
MOV r0,r1 // b
MOV r1,r2 // c
CALL r_mul // bc
PUSH r0 // .. a b bc
MOV r0,s2 // a
MOV r1,r3 // d
CALL r_mul // ad
POP r1 // bc; .. a b
CALL r_sub // D:=ad-bc
XCHG r0,s0 // b; .. a D
MOV r1,r5 // f
CALL r_mul // bf
PUSH r0 // .. a D bf
MOV r0,r4 // e
MOV r1,r3 // d
CALL r_mul // ed
POP r1 // bf; .. a D
CALL r_sub // Dx:=ed-bf
XCHG r0,s1 // a ; .. Dx D
MOV r1,r5 // f
CALL r_mul // af
PUSH r0 // .. Dx D af
MOV r0,r4 // e
MOV r1,r2 // c
CALL r_mul // ec
MOV r1,r0 // ec
POP r0 // af; .. Dx D
CALL r_sub // Dy:=af-ec
XCHG r0,s1 // Dx; .. Dy D
MOV r1,s0 // D
CALL r_div // x:=Dx/D
XCHG r0,s1 // Dy; .. x D
POP r1 // D ; .. x
CALL r_div // y:=Dy/D
MOV r1,r0 // y
POP r0 // x
RET

```

We have used 39 instructions: 11 one-byte, 17 two-byte (among them 5 "new" instructions), and 11 three-byte, for a total of $$11 \cdot 1+17 \cdot 2+11 \cdot 3=78$$ bytes. Somewhat paradoxically, the code size in bytes is slightly longer than in the previous case (cf. [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers)), contrary to what one might have expected. This is partially due to the fact that we have assumed two-byte encodings for "new" MOV and XCHG instructions involving the stack, similarly to the "old" instructions. Most existing architectures (such as x86-64) use longer encodings (maybe even twice as long) for their counterparts of our "new" move and exchange instructions compared to the "usual" register-register ones. Taking this into account, we see that we would have obtained here 83 bytes (versus 87 for the code in [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers) assuming three-byte encodings of new operations, and 88 bytes (versus 98) assuming four-byte encodings. This shows that, for two-address architectures without optimized encodings for register-stack move and exchange operations, $$m=16$$ preserved registers might result in slightly shorter code for some non-leaf functions, at the expense of leaf functions (cf. [$$\mathbf{C.2.3}$$](code-density-of-stack-and-register-machines.md#c23-case--all-registers-must-be-preserved) and [$$\mathbf{C.2.4}$$](code-density-of-stack-and-register-machines.md#c24-case--registers-r8r15-must-be-preserved)), which would become considerably longer.

### C.3.5. One-address register machine, $$m=0$$ preserved registers.

For our one-address register machine, we assume that new register-stack instructions work through the accumulator only. Therefore, we have three new instructions, LD $$\mathrm{s}(j)$$ (equivalent to MOV $$r 0, \mathrm{~s}(j)$$ of two-address machines), $$\mathrm{ST} \mathrm{s}(j)$$ (equivalent to MOV $$\mathrm{s}(j), \mathrm{rO}$$ ), and XCHG $$\mathrm{s}(j)$$ (equivalent to XCHG rO, $$\mathrm{s}(j))$$. To make the comparison with two-address machines more interesting, we assume one-byte encodings for these new instructions, even though this would consume $$48 / 256=3 / 16$$ of the opcode space.

By adapting the code provided in [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers) to the one-address machine, we obtain the following:

```
PUSH r4 // STACK: e
PUSH r1 // STACK: e b
PUSH r0 // .. e b a
PUSH r6 // .. e b a f
PUSH r2 // .. e b a f c
PUSH r3 // .. e b a f c d
LD s1 // r0:=c
XCHG r1 // r0:=b, r1:=c
CALL r_mul // bc
PUSH r0 // .. e b a f c d bc
LD s1 // d
XCHG r1 // r1:=d
LD s4 // a
CALL r_mul // ad
POP r1 // bc; .. e b a f c d
CALL r_sub // D:=ad-bc
XCHG s4 // b ; .. e D a f c d
XCHG r1
LD s2 // f
XCHG r1 // r0:=b, r1:=f
CALL r_mul // bf
POP r1 // d ; .. e D a f c
PUSH r0 // .. e D a f c bf
LD s5 // e
CALL r_mul // ed
POP r1 // bf; .. e D a f c
CALL r_sub // Dx:=ed-bf
XCHG s4 // e ; .. Dx D a f c
POP r1 // c ; .. Dx D a f
CALL r_mul // ec
XCHG s1 // a ; .. Dx D ec f
POP r1 // f ; .. Dx D ec
CALL r_mul // af
POP r1 // ec; .. Dx D
CALL r_sub // Dy:=af-ec
XCHG s1 // Dx; .. Dy D
POP r1 // D ; .. Dy
PUSH r1 // .. Dy D
CALL r_div // x:=Dx/D
XCHG s1 // Dy; .. x D
POP r1 // D ; .. x
CALL r_div // y:=Dy/D
XCHG r1 // r1:=y
POP r0 // r0:=x ; ..
RET
```

We have used 45 instructions: 34 one-byte and 11 three-byte, for a total of 67 bytes. Compared to the 76 bytes used by two- and three-address machines in [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers) we see that, again, the one-address register machine code may be denser than that of two-register machines, at the expense of utilizing more opcode space (just as shown in [$$\mathbf{C.2}$$](code-density-of-stack-and-register-machines.md#c2-comparison-of-machine-code-for-sample-leaf-func--tion)). However, this time the extra 3/16 of the opcode space was used for data manipulation instructions, which do not depend on specific arithmetic operations or user functions invoked.

### C.3.6. One-address register machine, $$m=8$$ preserved registers.

As explained in [$$\mathbf{C.3.3}$$](code-density-of-stack-and-register-machines.md#c33) the preservation of r8-r15 between subroutine calls does not improve the size of our previously written code, so the one-address machine will use for $$m=8$$ the same code provided in [$$\mathbf{C.3.5}$$](code-density-of-stack-and-register-machines.md#c35-one-address-register-machine-preserved-registers).

### C.3.7. One-address register machine, $$m=16$$ preserved registers.

We simply adapt the code provided in [$$\mathbf{C.3.4}$$](code-density-of-stack-and-register-machines.md#c34-three-address-and-two-address-register-machines-preserved-registers) to the one-address register machine:

```
PUSH r1 // STACK: a b
MOV r0,r1 // b
MOV r1,r2 // c
CALL r_mul // bc
PUSH r0 // .. a b bc
LD s2 // a
MOV r1,r3 // d
CALL r_mul // ad
POP r1 // bc; .. a b
CALL r_sub // D:=ad-bc
XCHG s0 // b; .. a D
MOV r1,r5 // f
CALL r_mul // bf
PUSH r0 // .. a D bf
MOV r0,r4 // e
MOV r1,r3 // d
CALL r_mul // ed
POP r1 // bf; .. a D
CALL r_sub // Dx:=ed-bf
XCHG s1 // a ; .. Dx D
MOV r1,r5 // f
CALL r_mul // af
PUSH r0 // .. Dx D af
MOV r0,r4 // e
MOV r1,r2 // c
CALL r_mul // ec
MOV r1,r0 // ec
POP r0 // af; .. Dx D
CALL r_sub // Dy:=af-ec
XCHG s1 // Dx; .. Dy D
POP r1 // D ; .. Dy
PUSH r1 // .. Dy D
CALL r_div // x:=Dx/D
XCHG s1 // Dy; .. x D
POP r1 // D ; .. x
CALL r_div // y:=Dy/D
MOV r1,r0 // y
POP r0 // x
RET
```

We have used 40 instructions: 18 one-byte, 11 two-byte, and 11 three-byte, for a total of $$18 \cdot 1+11 \cdot 2+11 \cdot 3=73$$ bytes.

### C.3.8. Stack machine with basic stack primitives.

We reuse the code provided in [$$\mathbf{C.1.5}$$](code-density-of-stack-and-register-machines.md#c15-stack-machine-with-basic-stack-primitives) simply replacing arithmetic primitives (VM instructions) with subroutine calls. The only substantive modification is the insertion of the previously optional XCHG s1 before the third multiplication, because even an optimizing compiler cannot now know whether CALL r\_mul is a commutative operation. We have also used the "tail recursion optimization" by replacing the final CALL $$r_{-}$$div followed by RET with JMP $$r_{-} d i v$$.

```
PUSH s5 // a b c d e f a
PUSH s3 // a b c d e f a d
CALL r_mul // a b c d e f ad
PUSH s5 // a b c d e f ad b
PUSH s5 // a b c d e f ad b c
CALL r_mul // a b c d e f ad bc
CALL r_sub // a b c d e f ad-bc
XCHG s3 // a b c ad-bc e f d
PUSH s2 // a b c ad-bc e f d e
XCHG s1 // a b c ad-bc e f e d
CALL r_mul // a b c ad-bc e f ed
XCHG s5 // a ed c ad-bc e f b
PUSH s1 // a ed c ad-bc e f b f
CALL r_mul // a ed c ad-bc e f bf
XCHG s1,s5 // a f c ad-bc e ed bf
CALL r_sub // a f c ad-bc e ed-bf
XCHG s3 // a f ed-bf ad-bc e c
CALL r_mul // a f ed-bf ad-bc ec
XCHG s3 // a ec ed-bf ad-bc f
XCHG s1,s4 // ad-bc ec ed-bf a f
CALL r_mul // D ec Dx af
XCHG s1 // D ec af Dx
XCHG s2 // D Dx af ec
CALL r_sub // D Dx Dy
XCHG s1 // D Dy Dx
PUSH s2 // D Dy Dx D
CALL r_div // D Dy x
XCHG s2 // x Dy D
JMP r_div // x y
```

We have used 29 instructions; assuming one-byte encodings for all stack operations, and three-byte encodings for CALL and JMP instructions, we end up with 51 bytes.

### C.3.9. Stack machine with compound stack primitives.

We again reuse the code provided in [$$\mathbf{C.1.7}$$](code-density-of-stack-and-register-machines.md#c17-stack-machine-with-compound-stack-primitives-and-manually-optimized-code), replacing arithmetic primitives with subroutine calls and making the tail recursion optimization:

```
PUSH2 s5,s2 // a b c d e f a d
CALL r_mul // a b c d e f ad
PUSH2 s5,s4 // a b c d e f ad b c
CALL r_mul // a b c d e f ad bc
CALL r_sub // a b c d e f ad-bc
PUXC s2,s3 // a b c ad-bc e f e d
CALL r_mul // a b c D e f ed
XCHG3 s6,s0,s5 // (same as XCHG s2,s6; XCHG s1,s0; XCHG s0,s5)
// e f c D a ed b
PUSH s5 // e f c D a ed b f
CALL r_mul // e f c D a ed bf
CALL r_sub // e f c D a ed-bf
XCHG s4 // e Dx c D a f
CALL r_mul // e Dx c D af
XCHG2 s4,s2 // D Dx af e c
CALL r_mul // D Dx af ec
CALL r_sub // D Dx Dy
XCPU s1,s2 // D Dy Dx D
CALL r_div // D Dy x
XCHG s2 // x Dy D
JMP r_div // x y
```

This code uses only 20 instructions, 9 stack-related and 11 control flowrelated (CALL and JMP), for a total of 48 bytes.

| Machine       |  m  | Operations | Operations | Operations | Code bytes | Code bytes |    Code bytes    | Opcode space |           Opcode space          |  Opcode space |
| ------------- | :-: | :--------: | :--------: | :--------: | :--------: | :--------: | :--------------: | :----------: | :-----------------------------: | :-----------: |
| -             |  -  |    data    |    cont.   |    total   |    data    |    cont.   |       total      |     data     |              arith              |     total     |
| 3-addr.       | 0,8 |     29     |     12     |     41     |     42     |     34     | $$\mathbf{7 6}$$ | $$35 / 256$$ |     $$\mathbf{3 4 / 2 5 6}$$    |  $$72 / 256$$ |
| 3-addr.       |  16 |     27     |     12     |     39     |     44     |     34     | $$\mathbf{7 8}$$ |              |                                 |               |
| 2-addr.       | 0,8 |     29     |     12     |     41     |     42     |     34     | $$\mathbf{7 6}$$ | $$37 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$44 / 256$$ |
| 2-addr.       |  16 |     27     |     12     |     39     |     44     |     34     | $$\mathbf{7 8}$$ |              |                                 |               |
| 1-addr.       | 0,8 |     33     |     12     |     45     |     33     |     34     | $$\mathbf{6 7}$$ |   $$/ 256$$  |     $$\mathbf{6 4 / 2 5 6}$$    | $$164 / 256$$ |
| 1-addr.       |  16 |     28     |     12     |     40     |     39     |     34     | $$\mathbf{7 3}$$ |              |                                 |               |
| stack (basic) |  -  |     18     |     11     |     29     |     18     |     33     | $$\mathbf{5 1}$$ | $$64 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$71 / 256$$ |
| stack (comp.) |  -  |      9     |     11     |     20     |     15     |     33     | $$\mathbf{4 8}$$ | $$84 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ |  $$91 / 256$$ |

#### Table 5:

A summary of machine code properties for hypothetical 3-address, 2-address, 1-address, and stack machines, generated for a sample non-leaf function (cf. C.3.1), assuming $$m$$ of the 16 registers must be preserved by called subroutines.

## C.4 Comparison of machine code for sample non-leaf function

[Table 5](code-density-of-stack-and-register-machines.md#table-5) summarizes the properties of machine code corresponding to the same source file provided in [$$\mathbf{C.3.1}$$](code-density-of-stack-and-register-machines.md#c31-sample-source-code-for-a-non-leaf-function). We consider only the "realistically" encoded three-address machines. Three-address and two-address machines have the same code density properties, but differ in the utilization of opcode space. The one-address machine, somewhat surprisingly, managed to produced shorter code than the two-address and three-address machines, at the expense of using up more than half of all opcode space. The stack machine is the obvious winner in this code density contest, without compromizing its excellent extendability (measured in opcode space used for arithmetic and other data transformation instructions).

### C.4.1. Combining with results for leaf functions.

It is instructive to compare this table with the results in C.2 for a sample leaf function, summarized in Table $$\mathbf{1}$$ (for $$m=0$$ preserved registers) and the very similar [Table 3](code-density-of-stack-and-register-machines.md#table-3) (for $$m=8$$ preserved registers), and, if one is still interested in case $$m=16$$ (which turned out to be worse than $$m=8$$ in almost all situations), also to [Table 2](code-density-of-stack-and-register-machines.md#table-2).

We see that the stack machine beats all register machines on non-leaf functions. As for the leaf functions, only the three-address machine with the "optimistic" encoding of arithmetic instructions was able to beat the stack machine, winning by $$15 \%$$, by compromising its extendability. However, the same three-address machine produces $$25 \%$$ longer code for non-leaf functions.

| Machine       |  m  | Operations | Operations | Operations | Code bytes | Code bytes |      Code bytes      |   Code bytes  |           Opcode space          |  Opcode space |
| ------------- | :-: | :--------: | :--------: | :--------: | :--------: | :--------: | :------------------: | :-----------: | :-----------------------------: | :-----------: |
| -             |  -  |    data    |    cont.   |    total   |    data    |    cont.   |         total        |      data     |              arith              |     total     |
| 3-addr.       | 0,8 |     29     |     12     |     41     |    35.5    |     34     | $$\mathbf{6 9 . 5}$$ | $$110 / 256$$ | $$\mathbf{2} / \mathbf{2 5 6}$$ | $$117 / 256$$ |
| 3-addr.       |  16 |     27     |     12     |     39     |    35.5    |     34     | $$\mathbf{6 9 . 5}$$ |               |                                 |               |
| 2-addr.       | 0,8 |     29     |     12     |     41     |    35.5    |     34     | $$\mathbf{6 9 . 5}$$ | $$110 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$117 / 256$$ |
| 2-addr.       |  16 |     27     |     12     |     39     |    35.5    |     34     | $$\mathbf{6 9 . 5}$$ |               |                                 |               |
| 1-addr.       | 0,8 |     33     |     12     |     45     |     33     |     34     |   $$\mathbf{6 7}$$   | $$112 / 256$$ | $$\mathbf{4} / \mathbf{2 5 6}$$ | $$119 / 256$$ |
| 1-addr.       |  16 |     28     |     12     |     40     |    33.5    |     34     | $$\mathbf{6 7 . 5}$$ |               |                                 |               |
| stack (basic) |  -  |     18     |     11     |     29     |     18     |     33     |   $$\mathbf{5 1}$$   |  $$64 / 256$$ |      $$\mathbf{4 / 2 5 6}$$     |  $$71 / 256$$ |
| stack (comp.) |  -  |      9     |     11     |     20     |     15     |     33     |   $$\mathbf{4 8}$$   |  $$84 / 256$$ |      $$\mathbf{4 / 2 5 6}$$     |  $$91 / 256$$ |

#### Table 6:

A summary of machine code properties for hypothetical 3-address, 2-address, 1-address, and stack machines, generated for a sample non-leaf function (cf. [$$\mathbf{C.3.1}$$](code-density-of-stack-and-register-machines.md#c31-sample-source-code-for-a-non-leaf-function)), assuming $$m$$ of the 16 registers must be preserved by called subroutines. This time we use fractions of bytes to encode instructions, enabling a fairer comparison. Otherwise, this table is similar to [Table 5](code-density-of-stack-and-register-machines.md#table-5).

If a typical program consists of a mixture of leaf and non-leaf functions in approximately equal proportion, then the stack machine will still win.

### C.4.2. A fairer comparison using a binary code instead of a byte code.

Similarly to [$$\mathbf{C.2.5}$$](code-density-of-stack-and-register-machines.md#c25-a-fairer-comparison-using-a-binary-code-instead-of-a-byte-code), we may offer a fairer comparison of different register machines and the stack machine by using arbitrary binary codes instead of byte codes to encode instructions, and matching the opcode space used for data manipulation and arithmetic instructions by different machines. The results of this modified comparison are summarized in Table 6 . We see that the stack machines still win by a large margin, while using less opcode space for stack/data manipulation.

### C.4.3. Comparison with real machines.

Note that our hypothetical register machines have been considerably optimized to produce shorter code than actually existing register machines; the latter are subject to other design considerations apart from code density and extendability, such as backward compatibility, faster instruction decoding, parallel execution of neighboring instructions, ease of automatically producing optimized code by compilers, and so on.

For example, the very popular two-address register architecture x86-64 produces code that is approximately twice as long as our "ideal" results for the two-address machines. On the other hand, our results for the stack machines are directly applicable to TVM, which has been explicitly designed with the considerations presented in this appendix in mind. Furthermore, the actual TVM code is even shorter (in bytes) than shown in [Table 5](code-density-of-stack-and-register-machines.md#table-5) because of the presence of the two-byte CALL instruction, allowing TVM to call up to 256 user-defined functions from the dictionary at c3. This means that one should subtract 10 bytes from the results for stack machines in [Table 5](code-density-of-stack-and-register-machines.md#table-5) if one wants to specifically consider TVM, rather than an abstract stack machine; this produces a code size of approximately 40 bytes (or shorter), almost half that of an abstract two-address or three-address machine.

### C.4.4. Automatic generation of optimized code.

An interesting point is that the stack machine code in our samples might have been generated automatically by a very simple optimizing compiler, which rearranges values near the top of the stack appropriately before invoking each primitive or calling a function as explained in [$$\mathbf{2.2.2}$$](stacks.md#2.2.2.-basic-stack-manipulation-primitives-suffice.) and [$$\mathbf{2.2.5}$$](stacks.md#2.2.5.-semantics-of-compound-stack-operations.) The only exception is the unimportant "manual" XCHG3 optimization described in [$$\mathbf{C.1.7}$$](code-density-of-stack-and-register-machines.md#c17-stack-machine-with-compound-stack-primitives-and-manually-optimized-code), which enabled us to shorten the code by one more byte.

By contrast, the heavily optimized (with respect to size) code for register machines shown in [$$\mathbf{C.3.2}$$](code-density-of-stack-and-register-machines.md#c32-three-address-and-two-address-register-machines-preserved-registers) and [$$\mathbf{C.3.3}$$](code-density-of-stack-and-register-machines.md)) is unlikely to be produced automatically by an optimizing compiler. Therefore, if we had compared compilergenerated code instead of manually-generated code, the advantages of stack machines with respect to code density would have been even more striking.
